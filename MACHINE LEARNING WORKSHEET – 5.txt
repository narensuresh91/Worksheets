MACHINE LEARNING WORKSHEET – 5

Q1. C) Both are equally proficient
Q2. B) entropy
Q3. D) ADASYN
Q4. B) 1 only
Q5. D) 1-3-2
Q6. C) K-Nearest Neighbours
Q7. C) CART can only create binary trees (a maximum of two children for a node), and CHAID can create multiway trees (more than two children for a node)
Q8. D) Lasso will cause some of the coefficients to become 0
Q9. B) remove only one of the features
Q10. A) Overfitting
Q11. When the feature values depict ranking, one hot encoding cannot be applied. In this case, label encoding can be used.
Q12. In case of imbalanced dataset,SMOTE, over sampling techniques can be applied. The dataset with smaller category is duplicated to balance the dataset.
Q13. SMOTE: Synthetic Minority Over sampling Technique (SMOTE) algorithm applies KNN approach where it selects K nearest neighbors, joins them and creates the synthetic samples in the space. The algorithm takes the feature vectors and its nearest neighbors, computes the distance between these vectors. The difference is multiplied by random number between (0, 1) and it is added back to feature. SMOTE algorithm is a pioneer algorithm and many other algorithms are derived from SMOTE.
     ADASYN:  ADAptive SYNthetic (ADASYN) is based on the idea of adaptively generating minority data samples according to their distributions using K nearest neighbor. The algorithm adaptively updates the distribution and there are no assumptions made for the underlying distribution of the data.  The algorithm uses Euclidean distance for KNN Algorithm. The key difference between ADASYN and SMOTE is that the former uses a density distribution, as a criterion to automatically decide the number of synthetic samples that must be generated for each minority sample by adaptively changing the weights of the different minority samples to compensate for the skewed distributions. The latter generates the same number of synthetic samples for each original minority sample.
Q14. GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters.
Q15. The metrics used to evaluate are accuracy,r2,MSE and RMSE. 
